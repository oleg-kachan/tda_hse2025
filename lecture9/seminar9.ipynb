{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f28d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gudhi\n",
    "!pip install giotto-tda\n",
    "!pip install ripser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca59bdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# gudhi\n",
    "import gudhi as gd\n",
    "from gudhi import bottleneck_distance\n",
    "from gudhi.hera import wasserstein_distance\n",
    "from gudhi.representations import BettiCurve, PersistenceImage, Landscape\n",
    "\n",
    "# giotto\n",
    "import gtda\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "# from gtda.diagrams import BettiCurve, PersistenceImage, PersistenceLandscape\n",
    "\n",
    "# ripser\n",
    "from ripser import lower_star_img\n",
    "from ripser import Rips\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5faee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gudhi_toarray(diagrams):\n",
    "    return np.array([[birth, death, dim] for (dim, (birth, death)) in diagrams])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bf396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagram_reshape(diagram):\n",
    "    zero_idx = np.where(diagram[:,2]==0)\n",
    "    one_idx = np.where(diagram[:,2]==1)\n",
    "    return diagram[zero_idx], diagram[one_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3957dc",
   "metadata": {},
   "source": [
    "## Persistence diagrams\n",
    "\n",
    "Topology studies data invariant to continous transformations, so topological invariants like (persistent) homology will not change under such class of transformations.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Apply rotation and dilation transformations to copy of original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_circles(n_samples=75*2, noise=0.1, random_state=0)\n",
    "X = X[y==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22dcf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.radians(30)\n",
    "c, s = np.cos(theta), np.sin(theta)\n",
    "R = np.array(((c,-s), (s, c)))\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed data\n",
    "X_transformed = np.copy(X)\n",
    "X_transformed[:,0] = X[:,0] * 2/3\n",
    "X_transformed = np.dot(X_transformed, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a180eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4.5))\n",
    "\n",
    "ax[0].set_title(\"Data\")\n",
    "ax[0].set_xlim(-1.5, 1.5)\n",
    "ax[0].set_ylim(-1.5, 1.5)\n",
    "ax[0].grid(linestyle=\"dotted\")\n",
    "ax[0].scatter(X[:,0], X[:,1], c=\"b\")\n",
    "\n",
    "ax[1].set_title(\"Transformed data\")\n",
    "ax[1].set_xlim(-1.5, 1.5)\n",
    "ax[1].set_ylim(-1.5, 1.5)\n",
    "ax[1].grid(linestyle=\"dotted\")\n",
    "ax[1].scatter(X_transformed[:,0], X_transformed[:,1], c=\"b\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b896d3",
   "metadata": {},
   "source": [
    "### Persistent diagram\n",
    "\n",
    "Compute persistence diagrams of the the Vietoris-Rips filtration of point cloud data\n",
    "\n",
    "#### GUDHI\n",
    "\n",
    "The GUDHI (Geometry Understanding in Higher Dimensions) library is a generic open source C++ library, with a Python interface for Topological Data Analysis. The library offers state-of-the-art data structures and algorithms to construct simplicial complexes and compute persistent homology.  \n",
    "https://gudhi.inria.fr/python/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e8f14e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vr_gudhi = gd.RipsComplex(points=X).create_simplex_tree(max_dimension=2)\n",
    "diagram_gudhi = vr_gudhi.persistence()\n",
    "diagram_gudhi # list of tuples (dim, (birth, death))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c69fd1",
   "metadata": {},
   "source": [
    "\n",
    "#### Giotto\n",
    "\n",
    "giotto-tda: A Topological Data Analysis Toolkit for Machine Learning and Data Exploration  \n",
    "Tauzin et al. _Journal of Machine Learning Research (2021)_  \n",
    "https://giotto-ai.github.io/gtda-docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d32d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vr_giotto = gtda.homology.VietorisRipsPersistence(homology_dimensions=[0, 1])\n",
    "diagram_giotto = vr_giotto.fit_transform(np.expand_dims(X, 0))\n",
    "diagram_giotto.shape # np.array of birth, death, dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c32d80",
   "metadata": {},
   "source": [
    "#### A list of list of numpy arrays convention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec90f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_reshape(gudhi_toarray(diagram_gudhi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48d7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagram_reshape(diagram_giotto[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80b908",
   "metadata": {},
   "source": [
    "#### Persistence diagram of data and its transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81af9cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vr = gd.RipsComplex(points=X).create_simplex_tree(max_dimension=2)\n",
    "X_transformed_vr = gd.RipsComplex(points=X_transformed).create_simplex_tree(max_dimension=2)\n",
    "\n",
    "X_diagram = diagram_reshape(gudhi_toarray(X_vr.persistence()))\n",
    "X_transformed_diagram = diagram_reshape(gudhi_toarray(X_transformed_vr.persistence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce0f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(10, 10))\n",
    "\n",
    "ax[0,0].set_title(\"Data $X$\")\n",
    "ax[0,0].set_xlim(-1.5, 1.5)\n",
    "ax[0,0].set_ylim(-1.5, 1.5)\n",
    "ax[0,0].grid(linestyle=\"dotted\")\n",
    "ax[0,0].scatter(X[:,0], X[:,1], c=\"b\", alpha=0.75)\n",
    "\n",
    "ax[0,1].set_title(\"Transformed data $T(X)$\")\n",
    "ax[0,1].set_xlim(-1.5, 1.5)\n",
    "ax[0,1].set_ylim(-1.5, 1.5)\n",
    "ax[0,1].grid(linestyle=\"dotted\")\n",
    "ax[0,1].scatter(X_transformed[:,0], X_transformed[:,1], c=\"b\", alpha=0.75)\n",
    "\n",
    "ax[1,0].set_title(\"Persistence diagram $D(X)$\")\n",
    "ax[1,0].set_xlim(-0.05, 1.6)\n",
    "ax[1,0].set_ylim(-0.05, 1.6)\n",
    "ax[1,0].scatter(X_diagram[0][:,0], X_diagram[0][:,1], c=\"b\", alpha=0.75, label=\"Dim 0\")\n",
    "ax[1,0].scatter(X_diagram[1][:,0], X_diagram[1][:,1], c=\"r\", alpha=0.75, label=\"Dim 1\")\n",
    "ax[1,0].legend(loc=\"lower right\")\n",
    "ax[1,0].plot([-0.05, 1.6], [-0.05, 1.6], c=\"k\", linestyle=\"dashed\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax[1,1].set_title(\"Persistence diagram $(D \\circ T)(X)$\")\n",
    "ax[1,1].set_xlim(-0.05, 1.6)\n",
    "ax[1,1].set_ylim(-0.05, 1.6)\n",
    "ax[1,1].scatter(X_transformed_diagram[0][:,0], X_transformed_diagram[0][:,1], c=\"b\", alpha=0.75, label=\"Dim 0\")\n",
    "ax[1,1].scatter(X_transformed_diagram[1][:,0], X_transformed_diagram[1][:,1], c=\"r\", alpha=0.75, label=\"Dim 1\")\n",
    "ax[1,1].legend(loc=\"lower right\")\n",
    "ax[1,1].plot([-0.05, 1.6], [-0.05, 1.6], c=\"k\", linestyle=\"dashed\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc850407",
   "metadata": {},
   "source": [
    "### Distances between persistence diagrams\n",
    "\n",
    "One can define the geometry on the space of persistent diagrams. Distances between persistence diagrams can be computed using the optimal transport approach, with $p$-Wasserstein distance as the metric. The _bollteneck distance_ is the $\\infty$-Wasserstein distance.\n",
    "\n",
    "#### Bottleneck distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ca10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_dim0 = bottleneck_distance(X_diagram[0][:,:2], X_transformed_diagram[0][:,:2])\n",
    "bottleneck_dim1 = bottleneck_distance(X_diagram[1][:,:2], X_transformed_diagram[1][:,:2])\n",
    "bottleneck_dim0, bottleneck_dim1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e8da76",
   "metadata": {},
   "source": [
    "#### p-Wasserstein distance, $p=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc3f152",
   "metadata": {},
   "outputs": [],
   "source": [
    "wasserstein_dim0 = wasserstein_distance(X_diagram[0][:,:2], X_transformed_diagram[0][:,:2], order=2)\n",
    "wasserstein_dim1 = wasserstein_distance(X_diagram[1][:,:2], X_transformed_diagram[1][:,:2], order=2)\n",
    "wasserstein_dim0, wasserstein_dim1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09315035",
   "metadata": {},
   "source": [
    "### Bottleneck distance stability\n",
    "\n",
    "The Bottleneck distance stability to small perturbations is theoretically proved. Let's show its empirically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d178d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 75*2\n",
    "\n",
    "n_noise_levels = 11\n",
    "n_repeats = 10\n",
    "\n",
    "# original data\n",
    "X, y = make_circles(n_samples=n_samples, noise=0.0)\n",
    "\n",
    "# original diagram\n",
    "X_vr = gd.RipsComplex(points=X).create_simplex_tree(max_dimension=2)\n",
    "X_diagram = diagram_reshape(gudhi_toarray(X_vr.persistence()))\n",
    "\n",
    "distances = np.zeros((n_noise_levels, n_repeats))\n",
    "X_noisy = np.zeros((n_noise_levels, n_samples, 2))\n",
    "\n",
    "for j in tqdm(range(n_repeats)):\n",
    "    for i, noise_level in enumerate(np.linspace(0, 0.3, n_noise_levels)):\n",
    "        \n",
    "        # i-th noisy data\n",
    "        X_noise, _ = make_circles(n_samples=n_samples, noise=noise_level, random_state=j)\n",
    "        X_noisy[i] = X_noise\n",
    "\n",
    "        # i-th noisy diagram\n",
    "        X_noise_vr = gd.RipsComplex(points=X_noise).create_simplex_tree(max_dimension=2)\n",
    "        X_diagram_noise = diagram_reshape(gudhi_toarray(X_noise_vr.persistence()))\n",
    "        \n",
    "        # bottleneck distance\n",
    "        bottleneck_dim0 = bottleneck_distance(X_diagram[0][:,:2], X_diagram_noise[0][:,:2])\n",
    "        bottleneck_dim1 = bottleneck_distance(X_diagram[1][:,:2], X_diagram_noise[1][:,:2])\n",
    "        distances[i,j] = 0.5 * bottleneck_dim0 + 0.5 * bottleneck_dim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3effdffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=11, figsize=(17,1.5))\n",
    "\n",
    "for j in range(n_noise_levels):\n",
    "    ax[j].set_title(\"{}\".format(np.linspace(0, 0.3, n_noise_levels)[j]))\n",
    "    ax[j].scatter(X_noisy[j,:,0], X_noisy[j,:,1], s=10, c=\"b\", alpha=0.75)\n",
    "    ax[j].set_xticks([])\n",
    "    ax[j].set_yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d72855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.set_xlabel(\"Noise level\")\n",
    "ax.set_ylabel(\"Bottleneck distance\")\n",
    "ax.set_xticks(range(n_noise_levels), np.linspace(0, 0.3, n_noise_levels))\n",
    "ax.grid(linestyle=\"dotted\")\n",
    "ax.plot(distances.mean(axis=1), c=\"b\")\n",
    "\n",
    "std = distances.std(axis=1)\n",
    "lower = distances.mean(axis=1) - std\n",
    "upper = distances.mean(axis=1) + std\n",
    "ax.fill_between(range(n_noise_levels), lower, upper, color='b', alpha=.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4159f",
   "metadata": {},
   "source": [
    "## Vectorizations of persistence diagrams\n",
    "\n",
    "#### Model data\n",
    "\n",
    "Persistence Diagrams with Linear Machine Learning Models  \n",
    "Obayashi et al. _Journal of Applied and Computational Topology (2018)_  \n",
    "https://arxiv.org/abs/1706.10082"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8932e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 300\n",
    "sigma1 = 4\n",
    "sigma2 = 2\n",
    "t = 0.01\n",
    "\n",
    "def generate(N, S, W=300, sigma1=4, sigma2=2, t=0.01, bins=64):\n",
    "\n",
    "    z = np.zeros((N, S, 2))\n",
    "    for n in range(N):\n",
    "        z[n, 0] = np.random.uniform(0, W, size=(2))\n",
    "        for s in range(S-1):\n",
    "            d_1 = np.random.normal(0, sigma1)\n",
    "            d_2 = np.random.normal(0, sigma1)\n",
    "            z[n, s+1, 0] = (z[n, s, 0] + d_1) % W\n",
    "            z[n, s+1, 1] = (z[n, s, 1] + d_2) % W\n",
    "\n",
    "    z_r = z.reshape(N*S, 2)\n",
    "    H, _, _ = np.histogram2d(z_r[:,0], z_r[:,1], bins=bins)\n",
    "    \n",
    "    G = sp.ndimage.gaussian_filter(H, sigma2)\n",
    "    G[G < t] = 0\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18778ca",
   "metadata": {},
   "source": [
    "#### Generate 100 images of model A and B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.zeros((100,64,64))\n",
    "y = np.concatenate((np.zeros(50), np.ones(50)), axis=0)\n",
    "\n",
    "# class A\n",
    "N = 150\n",
    "S = 20\n",
    "\n",
    "for n in range(50):\n",
    "    images[n] = generate(N, S)\n",
    "    \n",
    "# class B\n",
    "N = 250\n",
    "S = 10\n",
    "\n",
    "for n in range(50):\n",
    "    images[n+50] = generate(N, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5d5ff3",
   "metadata": {},
   "source": [
    "#### Compute persistence diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2949137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diags = []\n",
    "\n",
    "for i in range(images.shape[0]):\n",
    "    diags.append(lower_star_img(images[i])[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f5482",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(16, 4))\n",
    "\n",
    "ax[0].set_title(\"Image A[0]\")\n",
    "ax[0].imshow(images[0])\n",
    "\n",
    "ax[1].set_title(\"Persistence diagram A[0]\")\n",
    "ax[1].set_xlim(-0.05, 1.6)\n",
    "ax[1].set_ylim(-0.05, 1.6)\n",
    "ax[1].scatter(diags[0][:,0], diags[0][:,1], s=20, c=\"b\", alpha=0.75, label=\"Dim 0\")\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "ax[1].plot([-0.05, 1.6], [-0.05, 1.6], c=\"k\", linestyle=\"dashed\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax[2].set_title(\"Image B[0]\")\n",
    "ax[2].imshow(images[50])\n",
    "\n",
    "ax[3].set_title(\"Persistence diagram A[0]\")\n",
    "ax[3].set_xlim(-0.05, 1.6)\n",
    "ax[3].set_ylim(-0.05, 1.6)\n",
    "ax[3].scatter(diags[50][:,0], diags[50][:,1], s=20, c=\"b\", alpha=0.75, label=\"Dim 0\")\n",
    "ax[3].legend(loc=\"lower right\")\n",
    "ax[3].plot([-0.05, 1.6], [-0.05, 1.6], c=\"k\", linestyle=\"dashed\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04517445",
   "metadata": {},
   "source": [
    "## Vectorizations of persistence diagrams\n",
    "\n",
    "#### Betti curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553ecb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bc = BettiCurve(resolution=8, predefined_grid=np.linspace(0, 2, 20))\n",
    "betti_curve = bc.fit_transform([diags[0]])\n",
    "betti_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46c8ec9",
   "metadata": {},
   "source": [
    "#### Persistence landscapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11df1bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = Landscape(num_landscapes=3, resolution=20)\n",
    "persistence_landscapes = pl.fit_transform([diags[0]])\n",
    "persistence_landscapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec37f751",
   "metadata": {},
   "source": [
    "#### Persistence image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf804d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = PersistenceImage(bandwidth=0.1, resolution=[20, 20])\n",
    "persistence_image = pi.fit_transform([diags[0]])\n",
    "persistence_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9163db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=5, figsize=(16, 3))\n",
    "\n",
    "ax[0].set_title(\"Image\")\n",
    "ax[0].imshow(images[0])\n",
    "\n",
    "ax[1].set_title(\"Persistence diagram\")\n",
    "ax[1].set_xlim(-0.05, 1.6)\n",
    "ax[1].set_ylim(-0.05, 1.6)\n",
    "ax[1].scatter(diags[0][:,0], diags[0][:,1], s=20, c=\"b\", alpha=0.75, label=\"Dim 0\")\n",
    "ax[1].legend(loc=\"lower right\")\n",
    "ax[1].plot([-0.05, 1.6], [-0.05, 1.6], c=\"k\", linestyle=\"dashed\", linewidth=0.5, alpha=0.5)\n",
    "\n",
    "ax[2].set_title(\"Betti curve\")\n",
    "ax[2].plot(betti_curve[0], c=\"b\")\n",
    "\n",
    "ax[3].set_title(\"Persistence landscapes\")\n",
    "ax[3].plot(persistence_landscapes[0][0:20], c=\"r\")\n",
    "ax[3].plot(persistence_landscapes[0][20:40], c=\"g\")\n",
    "ax[3].plot(persistence_landscapes[0][40:60], c=\"b\")\n",
    "\n",
    "ax[4].set_title(\"Persistence image\")\n",
    "ax[4].imshow(persistence_image[0].reshape(20, 20))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afecc58",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "#### Betti curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8261d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BettiCurve(resolution=50, predefined_grid=np.linspace(0, 4, 50))\n",
    "X_betti_curves = bc.fit_transform(diags)\n",
    "X_betti_curves.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa38222",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50133496",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_curve_h0 = cross_val_score(clf, X_betti_curves, y, cv=cv)\n",
    "print(\"Accuracy, Betti curve, H0: {:.4f} ± {:.4f}\".format(np.mean(acc_curve_h0), np.std(acc_curve_h0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef58aa2",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a447a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = PCA().fit_transform(X_betti_curves)\n",
    "X_isomap = Isomap().fit_transform(X_betti_curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700a809",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2, figsize=(10, 4.5))\n",
    "ax[0].set_title(\"PCA\")\n",
    "ax[0].grid(linestyle=\"dotted\")\n",
    "ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y==0, cmap=\"bwr\")\n",
    "ax[1].set_title(\"Isomap\")\n",
    "ax[1].grid(linestyle=\"dotted\")\n",
    "ax[1].scatter(X_isomap[:, 0], X_isomap[:, 1], c=y==0, cmap=\"bwr\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cde0fea",
   "metadata": {},
   "source": [
    "## Learnable vectorizations of persistence diagrams\n",
    "\n",
    "Persistence diagram is a multiset of vectors $D = \\{(b_i, d_i, h_i)\\}_{i=1}^N$ where $b_i$, $d_i$ are the birth and death times of $i$-th topological feature of dimension $h_i$. The classic approach to introduce persistent diagrams to machine learning is related to distances and kernels defined on the space of diagrams, which takes $O(n^2)$ time to compute. Vectrorization schemes such as persistence [images](https://arxiv.org/abs/1507.06217), [landscapes](https://arxiv.org/abs/1501.00179) or [curves](https://arxiv.org/abs/1904.07768) reduce the time to $O(n)$, yet all of this approaches are more or less fixed.\n",
    "\n",
    "Trainable vectorization allows to learn vector representations of persistence diagrams, optimal w.r.t. the downstream task such as classification or regression. The simplest of such models, [Deep Sets](https://arxiv.org/abs/1703.06114) - $f: (\\mathbb{R}^3)^N \\rightarrow \\mathbb{R}^d$\n",
    "\n",
    "\\begin{equation}\n",
    "f(\\{x_1, \\dots, x_N\\}) = \\rho \\left( \\sum_{i=1}^N \\phi(x_i) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "consists of an encoder $\\phi_\\theta: \\mathbb{R}^3 \\rightarrow \\mathbb{R}^D$ mapping each diagram point $x_i = (b_i, d_i, h_i)$, with parameters $\\theta$ shared between points, a permutation invariant pooling operation $(\\cdot): (\\mathbb{R}^D)^N \\rightarrow \\mathbb{R}^D$ to obtain a representation of a diagram at whole (particulary for Deep Sets - sum pooling), and a decoder $\\rho: \\mathbb{R}^D \\rightarrow \\mathbb{R}^d$ which further transforms the diagram representation. It was [shown](https://arxiv.org/abs/1904.09378) that certain combinations of encoder/pooling/decoder correspond to the fixed representation schemes of persistence diagrams.\n",
    "\n",
    "Deep sets encoder vectorizes each single point independently and does not consider the interdependence between points in the diagram. Thus, the self-attention block from the Transformer model which allows to capture those dependencies is a natural plug-in replacement to the encoder $\\phi$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\Phi_{ATTN}(\\{x_1, \\dots, x_N\\}) = \\left(\\frac{(\\mathbf{W}_q \\mathbf{X})(\\mathbf{W}_k \\mathbf{X})^T}{\\sqrt{D}} \\right)\\mathbf{W}_v\\mathbf{X},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\Phi_{ATTN}: (\\mathbb{R}^3)^N \\rightarrow (\\mathbb{R}^D)^N$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0fb490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_orbit(point_0, r, n=300):\n",
    "    \n",
    "    X = np.zeros([n, 2])\n",
    "    \n",
    "    xcur, ycur = point_0[0], point_0[1]\n",
    "    \n",
    "    for idx in range(n):\n",
    "        xcur = (xcur + r * ycur * (1. - ycur)) % 1\n",
    "        ycur = (ycur + r * xcur * (1. - xcur)) % 1\n",
    "        X[idx, :] = [xcur, ycur]\n",
    "    \n",
    "    return X\n",
    "\n",
    "def generate_orbits(m, rs=[2.5, 3.5, 4.0, 4.1, 4.3], n=300, random_state=None):\n",
    "    \n",
    "    # m orbits, each of n points of dimension 2\n",
    "    orbits = np.zeros((m * len(rs), n, 2))\n",
    "    \n",
    "    # for each r\n",
    "    for j, r in enumerate(rs):\n",
    "\n",
    "        # initial points\n",
    "        points_0 = random_state.uniform(size=(m,2))\n",
    "\n",
    "        for i, point_0 in enumerate(points_0):\n",
    "            orbits[j*m + i] = generate_orbit(points_0[i], rs[j])\n",
    "            \n",
    "    return orbits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b746b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = np.random.RandomState(42)\n",
    "X_orbit5k = generate_orbits(1000, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06b197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(16, 6), dpi=200)\n",
    "for i in range(2):\n",
    "    for j in range(5):\n",
    "        if i==0:\n",
    "            ax[i,j].set_title(\"Class {}\".format(j+1))\n",
    "        ax[i,j].scatter(X_orbit5k[j*1000+i,:,0], X_orbit5k[j*1000+i,:,1], s=10, c=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a7122",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pc = np.concatenate((X_orbit5k[2000:3000], X_orbit5k[4000:5000]))\n",
    "y = np.concatenate((np.zeros(1000), np.ones(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_pd(diagrams):\n",
    "    pd = np.zeros((0, 3))\n",
    "\n",
    "    for k, diagram_k in enumerate(diagrams):\n",
    "        diagram_k = diagram_k[~np.isinf(diagram_k).any(axis=1)] # filter infs  \n",
    "        diagram_k = np.concatenate((diagram_k, k * np.ones((diagram_k.shape[0], 1))), axis=1)\n",
    "        pd = np.concatenate((pd, diagram_k))\n",
    "\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4234ffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "vr = Rips()\n",
    "\n",
    "for x_pc in tqdm(X_pc):\n",
    "    diagram = conv_pd(vr.fit_transform(x_pc))\n",
    "    X.append(diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12b45f8",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bba2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Orbit2kDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    tmp_pd, _ = data[0]\n",
    "    \n",
    "    n_batch = len(data)\n",
    "    n_features_pd = tmp_pd.shape[1]\n",
    "    n_points_pd = max(len(pd) for pd, _ in data)\n",
    "    inputs_pd = np.zeros((n_batch, n_points_pd, n_features_pd), dtype=float)\n",
    "    labels = np.zeros(len(data))\n",
    "    \n",
    "    for i, (pd, label) in enumerate(data):\n",
    "        inputs_pd[i][:len(pd)] = pd\n",
    "        labels[i] = label\n",
    "    \n",
    "    return torch.Tensor(inputs_pd), torch.Tensor(labels).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209fd6c7",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452b7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSets(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden_enc, n_out_enc, n_hidden_dec=16, n_out_dec=2):\n",
    "        super(DeepSets, self).__init__()\n",
    "        self.encoder = Encoder(n_in, n_hidden_enc, n_out_enc)\n",
    "        self.decoder = MLP(n_out_enc, n_hidden_dec, n_out_dec)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        z_enc = self.encoder(X)\n",
    "        z = self.decoder(z_enc)\n",
    "        return z\n",
    "    \n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = Linear(n_in, n_hidden)\n",
    "        self.linear2 = Linear(n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = relu(self.linear1(X))\n",
    "        X = self.linear2(X)\n",
    "        return X\n",
    "    \n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.mlp = MLP(n_in, n_hidden, n_out)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.mlp(X)\n",
    "        x = X.mean(dim=1) # aggregation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028c15e",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_repeats = 2\n",
    "n_epochs = 100\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "\n",
    "n_train, n_test = 1600, 400\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "dataset = Orbit2kDataset(X, y)\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # data init\n",
    "    dataset_train, dataset_test = random_split(dataset, [n_train, n_test])\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloader_test =  DataLoader(dataset_test, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = DeepSets(n_in=3, n_hidden_enc=16, n_out_enc=8)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Train\", \"Test\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for batch in dataloader_train:\n",
    "            loss_batch = criterion(model(batch[0]), batch[1])\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for batch in dataloader_train:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_train = correct / len(dataloader_train.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "\n",
    "        correct = 0\n",
    "        for batch in dataloader_test:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_test = correct / len(dataloader_test.dataset)\n",
    "        history[repeat_idx,epoch_idx,2] = accuracy_test\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train, accuracy_test))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(n_epochs)\n",
    "loss_ci1 = history.mean(axis=0)[:,0] - history.std(axis=0)[:,0]\n",
    "loss_ci2 = history.mean(axis=0)[:,0] + history.std(axis=0)[:,0]\n",
    "acc_train_ci1 = history.mean(axis=0)[:,1] - history.std(axis=0)[:,1]\n",
    "acc_train_ci2 = history.mean(axis=0)[:,1] + history.std(axis=0)[:,1]\n",
    "acc_test_ci1 = history.mean(axis=0)[:,2] - history.std(axis=0)[:,2]\n",
    "acc_test_ci2 = history.mean(axis=0)[:,2] + history.std(axis=0)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53355258",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 4.5), dpi=200)\n",
    "fig.suptitle(\"Persistence diagrams\", fontsize=14)\n",
    "ax[0].set_title(\"Loss\")\n",
    "ax[1].set_title(\"Accuracy\")\n",
    "ax[0].set_ylim(0.49, 0.71)\n",
    "ax[1].set_ylim(0.49, 0.81)\n",
    "ax[0].plot(history.mean(axis=0)[:,0], c=\"g\")\n",
    "ax[0].fill_between(x, loss_ci1, loss_ci2, color=\"g\", alpha=0.1)\n",
    "ax[1].plot(history.mean(axis=0)[:,1], c=\"r\", label=\"Train\")\n",
    "ax[1].plot(history.mean(axis=0)[:,2], c=\"b\", label=\"Test\")\n",
    "ax[1].fill_between(x, acc_train_ci1, acc_train_ci2, color=\"r\", alpha=0.1)\n",
    "ax[1].fill_between(x, acc_test_ci1, acc_test_ci2, color=\"b\", alpha=0.1)\n",
    "ax[1].legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa6e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
