{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0090e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import gudhi as gd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(precision=2, sci_mode=False, linewidth=100)\n",
    "np.set_printoptions(precision=2, suppress=True, linewidth=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4dc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gudhi_toarray(diagrams, replace_inf=True):\n",
    "    diagram = np.array([[birth, death, dim] for (dim, (birth, death)) in diagrams])\n",
    "    if replace_inf==True:\n",
    "        diagram = np.nan_to_num(diagram, posinf=-np.inf)\n",
    "        diagram = np.nan_to_num(diagram, neginf=np.max(diagram))\n",
    "    return diagram\n",
    "\n",
    "def diagram_reshape(diagram):\n",
    "    zero_idx = np.where(diagram[:,2]==0)\n",
    "    one_idx = np.where(diagram[:,2]==1)\n",
    "    return diagram[zero_idx], diagram[one_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea80268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagram(image, device, sublevel=True):\n",
    "    # get height and square image\n",
    "    h = int(np.sqrt(image.shape[0]))\n",
    "    image_sq = image.reshape((h,h))\n",
    "\n",
    "    # create complex\n",
    "    cmplx = gd.CubicalComplex(dimensions=(h, h), top_dimensional_cells=(sublevel*image))\n",
    "\n",
    "    # get pairs of critical simplices\n",
    "    cmplx.compute_persistence()\n",
    "    critical_pairs = cmplx.cofaces_of_persistence_pairs()\n",
    "    \n",
    "    # get essential critical pixel\n",
    "    bpx0_essential = critical_pairs[1][0][0] // h, critical_pairs[1][0][0] % h\n",
    "\n",
    "    # get critical pixels corresponding to critical simplices\n",
    "    try:\n",
    "        bpx0 = [critical_pairs[0][0][i][0] for i in range(len(critical_pairs[0][0]))]\n",
    "        dpx0 = [critical_pairs[0][0][i][1] for i in range(len(critical_pairs[0][0]))]\n",
    "    except IndexError:\n",
    "        bpx0 = []\n",
    "        dpx0 = []\n",
    "        \n",
    "    try:\n",
    "        bpx1 = [critical_pairs[0][1][i][0] for i in range(len(critical_pairs[0][1]))]\n",
    "        dpx1 = [critical_pairs[0][1][i][1] for i in range(len(critical_pairs[0][1]))]\n",
    "    except IndexError:\n",
    "        bpx1 = []\n",
    "        dpx1 = []\n",
    "    \n",
    "\n",
    "    flat_image = image_sq.flatten()\n",
    "    pd0_essential = torch.tensor([[image_sq[bpx0_essential], torch.max(image)]])\n",
    "\n",
    "    if (len(bpx0)!=0):\n",
    "        pdb0 = flat_image[bpx0][:, None]\n",
    "        pdd0 = flat_image[dpx0][:, None]\n",
    "        pd0 = torch.Tensor(torch.hstack([pdb0, pdd0]))\n",
    "        pd0 = torch.vstack([pd0, pd0_essential.to(device)])\n",
    "    else:\n",
    "        pd0 = pd0_essential\n",
    "\n",
    "    if (len(bpx1)!=0):\n",
    "        pdb1 = flat_image[bpx1][:, None]\n",
    "        pdd1 = flat_image[dpx1][:, None]\n",
    "        pd1 = torch.Tensor(torch.hstack([pdb1, pdd1]))\n",
    "    else:\n",
    "        pd1 = torch.zeros((1, 2))\n",
    "    \n",
    "    return pd0, pd1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea12960f",
   "metadata": {},
   "source": [
    "# Seminar 12: Topological data analysis of digital images\n",
    "\n",
    "## MNIST\n",
    "- sublevel filtration\n",
    "- directional filtration\n",
    "- PHT w/ directional filtration\n",
    "- PHT w/ convolutional filtraion (fully differentiable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76684c",
   "metadata": {},
   "source": [
    "#### Torch Dataset and collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add9c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistenceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, diagrams, y):\n",
    "        super().__init__()\n",
    "        \n",
    "        # get diagrams as list of tensors\n",
    "        D = torch.ones([len(diagrams), max(map(len, diagrams))+1, 3]) * torch.inf  \n",
    "        for i, dgm in enumerate(diagrams):\n",
    "            D[i,:len(dgm)] = dgm\n",
    "\n",
    "        # cut to the largest diagram accross all dataset\n",
    "        max_len = torch.argmax(D[:,:,0], axis=1).max()\n",
    "        D = D[:,:max_len+1] # leave at least one inf value!\n",
    "            \n",
    "        self.D = D\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.D[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "\n",
    "    # get len of a batch and len of each diagram in a batch\n",
    "    n_batch = len(batch)\n",
    "    d_lengths = [int(torch.argmax(D[:,0])) for D, y_ in batch]\n",
    "    \n",
    "    # set batch tensor to the max length of a diagram in a batch\n",
    "    Ds = torch.ones([n_batch, max(d_lengths), 3]) * 0.\n",
    "    D_masks = torch.zeros([n_batch, max(d_lengths)]).bool()\n",
    "    ys = torch.zeros(n_batch).long()\n",
    "    \n",
    "    # populate diagrams, their masks, and targets\n",
    "    for i, (D, y) in enumerate(batch):\n",
    "        Ds[i][:d_lengths[i]] = D[:d_lengths[i]]\n",
    "        D_masks[i][d_lengths[i]:] = True\n",
    "        ys[i] = y\n",
    "    \n",
    "    return Ds, D_masks, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446d337a",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef772349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistentHomologyTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_in=3, d_out=5, d_model=16, d_hidden=32, num_heads=2, num_layers=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.linear_in = nn.Linear(d_in, d_model)\n",
    "        self.ln = nn.LayerNorm(d_in)\n",
    "        el = nn.TransformerEncoderLayer(d_model, num_heads, d_hidden, dropout, batch_first=True, activation=F.gelu)\n",
    "        self.encoder = nn.TransformerEncoder(el, num_layers)\n",
    "        self.linear_out = nn.Linear(d_model, d_out)\n",
    "        \n",
    "    def _masked_mean(self, X, mask):\n",
    "        X_masked = X * ~mask.unsqueeze(-1)\n",
    "        n_masks = torch.sum(~mask, axis=1)\n",
    "        X_masked_mean = torch.sum(X_masked, axis=1) / n_masks.unsqueeze(-1)\n",
    "        return X_masked_mean\n",
    "        \n",
    "    def forward(self, X, mask):\n",
    "        X = self.linear_in(self.ln(X))\n",
    "        X = self.encoder(X, src_key_padding_mask=mask)\n",
    "        X = self._masked_mean(X, mask)\n",
    "        X = self.linear_out(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e528fb",
   "metadata": {},
   "source": [
    "### Sublevel filtration\n",
    "\n",
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b6dfde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(), Normalize(0.0, 1.0)])\n",
    "dataset = MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "targets = dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce89d6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:44<00:00, 225.64it/s]\n"
     ]
    }
   ],
   "source": [
    "X_sublevel = []\n",
    "\n",
    "for image, y_ in tqdm(dataset):\n",
    "\n",
    "    # create cubical complex\n",
    "    cmplx = gd.CubicalComplex(dimensions=image.shape, top_dimensional_cells=image.flatten())\n",
    "\n",
    "    # get pairs of critical simplices\n",
    "    cmplx.compute_persistence()\n",
    "    X_sublevel.append(torch.tensor(gudhi_toarray(cmplx.persistence())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb9a3c7",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29bd0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 Loss   Acc   \n",
      "  0 2.2827 0.1506\n",
      "  1 2.2377 0.1620\n",
      "  2 2.2237 0.1648\n",
      "  3 2.2201 0.1806\n",
      "  4 2.2128 0.1930\n",
      "  5 2.1988 0.1882\n",
      "  6 2.1886 0.2053\n",
      "  7 2.1648 0.2168\n",
      "  8 2.1408 0.2170\n",
      "  9 2.1201 0.2387\n",
      " 10 2.0994 0.2490\n",
      " 11 2.0845 0.2513\n",
      " 12 2.0583 0.2311\n",
      " 13 2.0512 0.2318\n",
      " 14 2.0356 0.2754\n",
      " 15 2.0379 0.2533\n",
      " 16 2.0038 0.2649\n",
      " 17 1.9894 0.2117\n",
      " 18 1.9931 0.2856\n",
      " 19 1.9753 0.2918\n",
      " 20 1.9810 0.2770\n",
      " 21 1.9670 0.2847\n",
      " 22 1.9665 0.2961\n",
      " 23 1.9583 0.2926\n",
      " 24 1.9553 0.2732\n",
      " 25 1.9384 0.2876\n",
      " 26 1.9471 0.2998\n",
      " 27 1.9404 0.3006\n",
      " 28 1.9342 0.3056\n",
      " 29 1.9261 0.3053\n",
      " 30 1.9097 0.3030\n",
      " 31 1.9049 0.3133\n",
      " 32 1.9060 0.2927\n",
      " 33 1.9173 0.2933\n",
      " 34 1.8988 0.2969\n",
      " 35 1.8961 0.2854\n",
      " 36 1.9121 0.2976\n",
      " 37 1.8964 0.3113\n",
      " 38 1.8805 0.3169\n",
      " 39 1.8783 0.3099\n",
      " 40 1.8907 0.2672\n",
      " 41 1.8852 0.3218\n",
      " 42 1.8718 0.3135\n",
      " 43 1.8929 0.3125\n",
      " 44 1.8781 0.3061\n",
      " 45 1.8811 0.2997\n",
      " 46 1.8686 0.3105\n",
      " 47 1.8640 0.3213\n",
      " 48 1.8808 0.3014\n",
      " 49 1.8715 0.3106\n",
      "\n",
      "CPU times: user 4min 12s, sys: 28.6 s, total: 4min 41s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_repeats = 1\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # randomness\n",
    "    seed = repeat_idx\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # random state\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # data init\n",
    "    dataset = PersistenceDataset(X_sublevel, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = PersistentHomologyTransformer(d_in=3, d_out=10)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Acc\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for X, mask, y in dataloader:\n",
    "            loss_batch = criterion(model(X, mask), y)\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for X, mask, y in dataloader:\n",
    "            y_hat = model(X, mask).argmax(dim=1)\n",
    "            correct += int((y_hat == y).sum())\n",
    "        accuracy_train = correct / len(dataloader.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e168eb6",
   "metadata": {},
   "source": [
    "### Direction filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c96560",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "diagrams_all_dir = pickle.load(open(\"./data/MNIST_D_test_dir.pkl\", \"rb\"))\n",
    "targets = dataset.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39a50eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 0\n",
    "\n",
    "diagrams_dir = []\n",
    "for diagram_dir in diagrams_all_dir:\n",
    "    dir_idx = diagram_dir[:,-1] == direction\n",
    "    diagrams_dir.append(diagram_dir[dir_idx,:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "527f92f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # get len of a batch and len of each diagram in a batch\n",
    "    n_batch = len(batch)\n",
    "    d_lengths = [int(torch.argmax(D[:,0])) for D, y_ in batch]\n",
    "    \n",
    "    # set batch tensor to the max length of a diagram in a batch\n",
    "    Ds = torch.ones([n_batch, max(d_lengths), 3]) * 0.\n",
    "    D_masks = torch.zeros([n_batch, max(d_lengths)]).bool()\n",
    "    ys = torch.zeros(n_batch).long()\n",
    "    \n",
    "    # populate diagrams, their masks, and targets\n",
    "    for i, (D, y) in enumerate(batch):\n",
    "        Ds[i][:d_lengths[i]] = D[:d_lengths[i]]\n",
    "        D_masks[i][d_lengths[i]:] = True\n",
    "        ys[i] = y\n",
    "        \n",
    "    # masked normalize\n",
    "    for i, (D, y) in enumerate(batch):\n",
    "        Ds[i][~D_masks[i],0] = (Ds[i][~D_masks[i],0] - 1.0662154048380699) / 0.48181154844016033\n",
    "        Ds[i][~D_masks[i],1] = (Ds[i][~D_masks[i],1] - 1.4032599645792931) / 0.3154062619965701\n",
    "        Ds[i][~D_masks[i],2] = (Ds[i][~D_masks[i],2] - 0.7567565129537418) / 0.4290408990479055\n",
    "    \n",
    "    return Ds, D_masks, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451da46",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9859d274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 Loss   Acc   \n",
      "  0 2.2007 0.2471\n",
      "  1 1.8998 0.3478\n",
      "  2 1.7431 0.4015\n",
      "  3 1.6303 0.4446\n",
      "  4 1.5513 0.4651\n",
      "  5 1.5060 0.4704\n",
      "  6 1.4723 0.4787\n",
      "  7 1.4572 0.4694\n",
      "  8 1.4448 0.4787\n",
      "  9 1.4310 0.4706\n",
      " 10 1.4211 0.4898\n",
      " 11 1.4156 0.4921\n",
      " 12 1.4048 0.4879\n",
      " 13 1.3969 0.4950\n",
      " 14 1.3953 0.5041\n",
      " 15 1.3888 0.5079\n",
      " 16 1.3853 0.5058\n",
      " 17 1.3847 0.5034\n",
      " 18 1.3745 0.5124\n",
      " 19 1.3655 0.5122\n",
      " 20 1.3661 0.5095\n",
      " 21 1.3583 0.5098\n",
      " 22 1.3603 0.5111\n",
      " 23 1.3578 0.5080\n",
      " 24 1.3595 0.5146\n",
      " 25 1.3421 0.5164\n",
      " 26 1.3488 0.5137\n",
      " 27 1.3455 0.5053\n",
      " 28 1.3453 0.5118\n",
      " 29 1.3402 0.5223\n",
      " 30 1.3386 0.5239\n",
      " 31 1.3369 0.5199\n",
      " 32 1.3343 0.5252\n",
      " 33 1.3295 0.5246\n",
      " 34 1.3314 0.5270\n",
      " 35 1.3301 0.5247\n",
      " 36 1.3211 0.5217\n",
      " 37 1.3258 0.5165\n",
      " 38 1.3187 0.5293\n",
      " 39 1.3179 0.5271\n",
      " 40 1.3198 0.5192\n",
      " 41 1.3106 0.5205\n",
      " 42 1.3141 0.5221\n",
      " 43 1.3140 0.5236\n",
      " 44 1.3053 0.5171\n",
      " 45 1.3060 0.5191\n",
      " 46 1.3056 0.5337\n",
      " 47 1.3100 0.5324\n",
      " 48 1.3029 0.5335\n",
      " 49 1.2990 0.5228\n",
      "\n",
      "CPU times: user 8min 10s, sys: 1min 10s, total: 9min 21s\n",
      "Wall time: 2min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_repeats = 1\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # randomness\n",
    "    seed = repeat_idx\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # random state\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # data init\n",
    "    dataset = PersistenceDataset(diagrams_dir, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = PersistentHomologyTransformer(d_in=3, d_out=10)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Acc\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for X, mask, y in dataloader:\n",
    "            loss_batch = criterion(model(X, mask), y)\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for X, mask, y in dataloader:\n",
    "            y_hat = model(X, mask).argmax(dim=1)\n",
    "            correct += int((y_hat == y).sum())\n",
    "        accuracy_train = correct / len(dataloader.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18e9ec1",
   "metadata": {},
   "source": [
    "## Persistence Homology Transform\n",
    "\n",
    "### Direction filter\n",
    "\n",
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a0b0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersistenceTransformDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, diagrams, y, idx=None, eps=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # get diagrams as list of tensors\n",
    "        D = torch.ones([len(diagrams), max(map(len, diagrams))+1, 4]) * torch.inf\n",
    "\n",
    "        # select points according to eps and idx\n",
    "        for i, dgm in enumerate(diagrams):\n",
    "\n",
    "            # eps\n",
    "            if eps is not None:\n",
    "                eps_idx = (dgm[:,1] - dgm[:,0]) >= eps\n",
    "                dgm = dgm[eps_idx]\n",
    "\n",
    "            # idx\n",
    "            if idx is not None:\n",
    "                dgm_idx = torch.isin(dgm[:,-1], idx)\n",
    "                dgm = dgm[dgm_idx]\n",
    "                D[i,:len(dgm)] = dgm[:,:-1]\n",
    "            else:\n",
    "                D[i,:len(dgm)] = dgm[:,:-1]\n",
    "\n",
    "        # cut to the largest diagram accross all dataset\n",
    "        if idx is not None:\n",
    "            max_len = torch.argmax(D[:,:,0], axis=1).max()\n",
    "            D = D[:,:max_len+1] # leave at least one inf value!\n",
    "            \n",
    "        self.D = D\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.D[idx], self.y[idx]\n",
    "    \n",
    "    \n",
    "def collate_fn(batch):\n",
    "\n",
    "    # get len of a batch and len of each diagram in a batch\n",
    "    n_batch = len(batch)\n",
    "    d_lengths = [int(torch.argmax(D[:,0])) for D, y_ in batch]\n",
    "    \n",
    "    # set batch tensor to the max length of a diagram in a batch\n",
    "    Ds = torch.ones([n_batch, max(d_lengths), 4]) * 0.\n",
    "    D_masks = torch.zeros([n_batch, max(d_lengths)]).bool()\n",
    "    ys = torch.zeros(n_batch).long()\n",
    "    \n",
    "    # populate diagrams, their masks, and targets\n",
    "    for i, (D, y) in enumerate(batch):\n",
    "        Ds[i][:d_lengths[i]] = D[:d_lengths[i]]\n",
    "        D_masks[i][d_lengths[i]:] = True\n",
    "        ys[i] = y\n",
    "        \n",
    "    # masked normalize\n",
    "    for i, (D, y) in enumerate(batch):\n",
    "        Ds[i][~D_masks[i],0] = (Ds[i][~D_masks[i],0] - 1.0662154048380699) / 0.48181154844016033\n",
    "        Ds[i][~D_masks[i],1] = (Ds[i][~D_masks[i],1] - 1.4032599645792931) / 0.3154062619965701\n",
    "        Ds[i][~D_masks[i],2] = (Ds[i][~D_masks[i],2] - 0.7567565129537418) / 0.4290408990479055\n",
    "        Ds[i][~D_masks[i],3] = (Ds[i][~D_masks[i],3] - 11.803894241177744) / 11.236356222975049\n",
    "    \n",
    "    return Ds, D_masks, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669e8ef",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80a3c057",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 Loss   Acc   \n",
      "  0 2.1943 0.2571\n",
      "  1 1.7395 0.4293\n",
      "  2 1.4065 0.5139\n",
      "  3 1.2606 0.5548\n",
      "  4 1.1725 0.5959\n",
      "  5 1.1093 0.6001\n",
      "  6 1.0515 0.6355\n",
      "  7 1.0418 0.6268\n",
      "  8 0.9912 0.6499\n",
      "  9 0.9617 0.6485\n",
      " 10 0.9524 0.6657\n",
      " 11 0.9323 0.6792\n",
      " 12 0.9368 0.6563\n",
      " 13 0.9022 0.6548\n",
      " 14 0.8900 0.6798\n",
      " 15 0.8857 0.6697\n",
      " 16 0.8697 0.6860\n",
      " 17 0.8681 0.6848\n",
      " 18 0.8609 0.6908\n",
      " 19 0.8501 0.6695\n",
      " 20 0.8458 0.7002\n",
      " 21 0.8313 0.6997\n",
      " 22 0.8379 0.7089\n",
      " 23 0.8205 0.7099\n",
      " 24 0.8093 0.6804\n",
      " 25 0.8084 0.7175\n",
      " 26 0.8138 0.7182\n",
      " 27 0.7930 0.7234\n",
      " 28 0.7891 0.6980\n",
      " 29 0.7968 0.7319\n",
      " 30 0.7726 0.7239\n",
      " 31 0.7680 0.7162\n",
      " 32 0.7766 0.7298\n",
      " 33 0.7618 0.7079\n",
      " 34 0.7531 0.7292\n",
      " 35 0.7581 0.7303\n",
      " 36 0.7482 0.7243\n",
      " 37 0.7377 0.7222\n",
      " 38 0.7307 0.7354\n",
      " 39 0.7311 0.7313\n",
      " 40 0.7369 0.7205\n",
      " 41 0.7203 0.7508\n",
      " 42 0.7145 0.7427\n",
      " 43 0.7190 0.7611\n",
      " 44 0.6955 0.7685\n",
      " 45 0.6976 0.7578\n",
      " 46 0.6999 0.7411\n",
      " 47 0.6949 0.7573\n",
      " 48 0.6891 0.7534\n",
      " 49 0.6729 0.7589\n",
      "\n",
      "CPU times: user 14min 31s, sys: 1min 28s, total: 16min\n",
      "Wall time: 4min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_repeats = 1\n",
    "n_epochs = 50\n",
    "batch_size = 64\n",
    "lr = 0.001\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # randomness\n",
    "    seed = repeat_idx\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # random state\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # data init\n",
    "    dataset = PersistenceTransformDataset(diagrams_all_dir, targets, idx=torch.tensor([0, 3, 6]), eps=0.0)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # model init\n",
    "    model = PersistentHomologyTransformer(d_in=4, d_out=10) # increase input dim by 1\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Acc\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for X, mask, y in dataloader:\n",
    "            loss_batch = criterion(model(X, mask), y)\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for X, mask, y in dataloader:\n",
    "            y_hat = model(X, mask).argmax(dim=1)\n",
    "            correct += int((y_hat == y).sum())\n",
    "        accuracy_train = correct / len(dataloader.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca404e",
   "metadata": {},
   "source": [
    "### Convolutional filter\n",
    "\n",
    "#### Differentiability of persistent homology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978ff2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_diff = MNIST(\"./data\", train=True, download=True, transform=transform)\n",
    "dataloader_diff = DataLoader(dataset_diff, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "debfcd0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images, _ = next(iter(dataloader_diff))\n",
    "images.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3661f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create complex\n",
    "cmplx = gd.CubicalComplex(dimensions=(28, 28), top_dimensional_cells=(images[0].flatten()))\n",
    "\n",
    "# reduce boundary matrix\n",
    "cmplx.compute_persistence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90760b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[array([], shape=(0, 2), dtype=int64),\n",
       "  array([[158, 159],\n",
       "         [317, 289],\n",
       "         [325, 297],\n",
       "         [381, 353],\n",
       "         [489, 488],\n",
       "         [214, 186],\n",
       "         [298, 242],\n",
       "         [372, 344],\n",
       "         [429, 400],\n",
       "         [436, 408],\n",
       "         [487, 458],\n",
       "         [490, 462],\n",
       "         [463, 464],\n",
       "         [546, 518],\n",
       "         [602, 574],\n",
       "         [687, 630]])],\n",
       " [array([783])]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# critical pixels\n",
    "critical_pairs = cmplx.cofaces_of_persistence_pairs()\n",
    "critical_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9ffea1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (0.0, 1.0)),\n",
       " (1, (0.7568627595901489, 0.9882352948188782)),\n",
       " (1, (0.95686274766922, 0.9960784316062927)),\n",
       " (1, (0.9882352948188782, 0.9960784316062927)),\n",
       " (1, (0.9882352948188782, 0.9960784316062927)),\n",
       " (1, (0.9921568632125854, 1.0)),\n",
       " (1, (0.9882352948188782, 0.9921568632125854)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9882352948188782, 0.9921568632125854)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9882352948188782, 0.9921568632125854)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9921568632125854, 0.9960784316062927)),\n",
       " (1, (0.9882352948188782, 0.9921568632125854)),\n",
       " (0, (0.0, inf))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persistence diagram\n",
    "pd = cmplx.persistence()\n",
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "622b6cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persistence_diagram(image):\n",
    "\n",
    "    h, w = image.shape\n",
    "    img_flat = image.flatten()\n",
    "\n",
    "    ccomplex = gd.CubicalComplex(\n",
    "        dimensions = (h, w), \n",
    "        top_dimensional_cells=img_flat\n",
    "    )\n",
    "    \n",
    "    # get pairs of critical simplices\n",
    "    ccomplex.compute_persistence()\n",
    "    critical_pairs = ccomplex.cofaces_of_persistence_pairs()\n",
    "\n",
    "    # get essential critical pixels (never vanish)\n",
    "    essential_features = critical_pairs[1][0]\n",
    "\n",
    "    # 0-homology image critical pixels\n",
    "    try:\n",
    "        critical_pairs_0 = critical_pairs[0][0]\n",
    "    except:\n",
    "        critical_pairs_0 = np.empty((0, 2))\n",
    "    critical_0_ver_ind = critical_pairs_0 // w\n",
    "    critical_0_hor_ind = critical_pairs_0 % w\n",
    "    critical_pixels_0 = np.stack([critical_0_ver_ind, critical_0_hor_ind], axis=2)\n",
    "\n",
    "    # 0-homology essential pixels (ends with last added pixel)\n",
    "    last_pixel = torch.argmax(image).item()\n",
    "    essential_pixels_0 = np.array([[essential_features[0] // w, essential_features[0] % w], [last_pixel // w, last_pixel % 4]])[np.newaxis, ...]\n",
    "    critical_pixels_0 = np.vstack([critical_pixels_0, essential_pixels_0])\n",
    "\n",
    "    # 0-homology persistance diagram\n",
    "    pd0 = image[critical_pixels_0[:, :, 0].flatten(), critical_pixels_0[:, :, 1].flatten()].reshape((critical_pixels_0.shape[0], 2))\n",
    "\n",
    "    # 1-homology image critical pixels\n",
    "    try:\n",
    "        critical_pairs_1 = critical_pairs[0][1]\n",
    "    except:\n",
    "        critical_pairs_1 = np.empty((0, 2))\n",
    "    critical_1_ver_ind = critical_pairs_1 // w\n",
    "    critical_1_hor_ind = critical_pairs_1 % w\n",
    "    critical_pixels_1 = np.stack([critical_1_ver_ind, critical_1_hor_ind], axis=2)\n",
    "\n",
    "    # 1-homology persistance diagram\n",
    "    pd1 = image[critical_pixels_1[:, :, 0].flatten(), critical_pixels_1[:, :, 1].flatten()].reshape((critical_pixels_1.shape[0], 2))\n",
    "\n",
    "    return pd0, pd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9887a566",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePersistence(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(3, 3))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X_conv = self.conv(X)\n",
    "        \n",
    "        peristence_diagrams = []\n",
    "        for i, x_conv in enumerate(X_conv):\n",
    "            pd = persistence_diagram(x_conv[0])\n",
    "            peristence_diagrams.append(pd)\n",
    "        \n",
    "        return peristence_diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b4ff4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[0.10, 0.11],\n",
       "          [0.16, 0.18],\n",
       "          [0.17, 0.21],\n",
       "          [0.06, 0.23],\n",
       "          [0.18, 0.23],\n",
       "          [0.08, 0.23],\n",
       "          [0.18, 0.23],\n",
       "          [0.02, 0.23]], grad_fn=<ViewBackward0>),\n",
       "  tensor([[0.23, 0.25],\n",
       "          [0.20, 0.27],\n",
       "          [0.50, 0.50],\n",
       "          [0.50, 0.51],\n",
       "          [0.45, 0.52],\n",
       "          [0.51, 0.53],\n",
       "          [0.54, 0.55],\n",
       "          [0.50, 0.56],\n",
       "          [0.49, 0.58],\n",
       "          [0.54, 0.67],\n",
       "          [0.67, 0.76],\n",
       "          [0.65, 0.78],\n",
       "          [0.23, 0.80]], grad_fn=<ViewBackward0>)),\n",
       " (tensor([[ 0.10,  0.13],\n",
       "          [ 0.13,  0.14],\n",
       "          [ 0.07,  0.15],\n",
       "          [ 0.15,  0.16],\n",
       "          [ 0.13,  0.16],\n",
       "          [ 0.17,  0.19],\n",
       "          [ 0.16,  0.19],\n",
       "          [ 0.15,  0.19],\n",
       "          [ 0.16,  0.20],\n",
       "          [ 0.05,  0.42],\n",
       "          [-0.04,  0.23]], grad_fn=<ViewBackward0>),\n",
       "  tensor([[0.20, 0.21],\n",
       "          [0.19, 0.21],\n",
       "          [0.21, 0.21],\n",
       "          [0.19, 0.22],\n",
       "          [0.23, 0.23],\n",
       "          [0.23, 0.25],\n",
       "          [0.23, 0.28],\n",
       "          [0.43, 0.44],\n",
       "          [0.49, 0.50],\n",
       "          [0.49, 0.52],\n",
       "          [0.51, 0.54],\n",
       "          [0.43, 0.56],\n",
       "          [0.49, 0.56],\n",
       "          [0.50, 0.58],\n",
       "          [0.50, 0.58],\n",
       "          [0.58, 0.58],\n",
       "          [0.59, 0.60],\n",
       "          [0.46, 0.62],\n",
       "          [0.51, 0.64],\n",
       "          [0.52, 0.65],\n",
       "          [0.64, 0.67],\n",
       "          [0.63, 0.67],\n",
       "          [0.67, 0.68],\n",
       "          [0.62, 0.68],\n",
       "          [0.49, 0.70],\n",
       "          [0.61, 0.71],\n",
       "          [0.71, 0.72],\n",
       "          [0.71, 0.74],\n",
       "          [0.63, 0.76],\n",
       "          [0.43, 0.80],\n",
       "          [0.23, 0.85]], grad_fn=<ViewBackward0>))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "persistent_homology = ImagePersistence()\n",
    "persistent_homology(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3216bff",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0862593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDiagram(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(ConvDiagram, self).__init__()\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        diagrams = []\n",
    "        for i in range(x.shape[0]):\n",
    "            res = diagram(x[i].flatten(), self.device)\n",
    "            for j in range(len(res)):\n",
    "                diagrams.append(torch.concatenate([res[j], torch.Tensor([[j, i] for _ in range(res[j].shape[0])]).to(self.device)], axis=1))\n",
    "        diagrams = torch.concatenate(diagrams)\n",
    "        return diagrams\n",
    "\n",
    "\n",
    "class Transformer(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hidden, n_out, seq_size=1024, nhead=2, num_layers=2, dim_feedforward=16):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embeddings = nn.Linear(n_in, n_hidden)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=n_hidden, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer=self.encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(seq_size, n_out)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embeddings(X)\n",
    "        X = self.transformer(X)\n",
    "        X = X.mean(dim=-1)\n",
    "        X = self.classifier(X)\n",
    "        X = X.softmax(dim=-1)\n",
    "        return X\n",
    "\n",
    "\n",
    "class TopologicalConvTransformer(nn.Module):\n",
    "    def __init__(self, n_in, n_conv, max_sequence, n_diag, n_hidden, n_out, nhead=2, num_layers=2, dim_feedforward=16, device='cuda'):\n",
    "        super(TopologicalConvTransformer, self).__init__()\n",
    "        \n",
    "        self.max_sequence = max_sequence\n",
    "        self.conv1 = nn.Conv2d(n_in, n_conv, 3)\n",
    "        self.conv2 = nn.Conv2d(n_conv, n_conv, 3)\n",
    "        self.conv3 = nn.Conv2d(n_conv, n_conv, 3)\n",
    "        self.bn1 = nn.BatchNorm2d(n_conv)\n",
    "        self.bn2 = nn.BatchNorm2d(n_conv)\n",
    "        self.bn3 = nn.BatchNorm2d(n_conv)\n",
    "        self.diagram = ConvDiagram(device)\n",
    "        self.transformer = Transformer(n_diag, n_hidden, n_out, max_sequence, nhead, num_layers, dim_feedforward)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        result = []\n",
    "        for i in range(xs.shape[0]):\n",
    "            x = xs[i][None, :, :] / 256\n",
    "            x = self.bn1(F.gelu(self.conv1(x)))\n",
    "            #x = self.bn2(F.gelu(self.conv2(x)))\n",
    "            #x = self.bn3(F.gelu(self.conv3(x)))\n",
    "            x = self.diagram(x)\n",
    "            if x.shape[0] > self.max_sequence:\n",
    "                x = x[:self.max_sequence]\n",
    "            x = F.pad(x, (0, 0, 0, self.max_sequence - x.shape[0]), \"constant\", 0)\n",
    "            x = self.transformer(x)\n",
    "            result.append(x[None, :])\n",
    "        result = torch.concatenate(result, axis=0)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dba33e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"n_in\": 1,\n",
    " \"n_conv\": 1,\n",
    " \"max_sequence\": 64,\n",
    " \"n_diag\": 4,\n",
    " \"n_hidden\": 32, \"n_out\": 10, \"nhead\": 2, \"num_layers\": 2, \"dim_feedforward\": 16, \"device\": \"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fc52926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.10, 0.10, 0.10, 0.11, 0.10, 0.10, 0.10, 0.11, 0.09, 0.09], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TopologicalConvTransformer(**kwargs)\n",
    "model(images)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a06d55a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 Loss   Acc   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:40\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/ml/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_repeats = 1\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "lr = 0.002\n",
    "\n",
    "history = np.zeros((n_repeats, n_epochs, 3))\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for repeat_idx in range(n_repeats):\n",
    "    \n",
    "    # randomness\n",
    "    seed = repeat_idx\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # random state\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # data init\n",
    "    dataset_conv = MNIST(\"./data\", train=False, download=True, transform=transform)\n",
    "    dataloader_conv = DataLoader(dataset_conv, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # model init\n",
    "    model = TopologicalConvTransformer(**kwargs)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    print(\"{:3} {:6} {:6}\".format(repeat_idx, \"Loss\", \"Acc\"))\n",
    "    \n",
    "    for epoch_idx in range(n_epochs):\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        \n",
    "        loss_epoch = []\n",
    "        for batch in dataloader_conv:\n",
    "            loss_batch = criterion(model(batch[0]), batch[1])\n",
    "            loss_batch.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_epoch.append(loss_batch.detach())\n",
    "        \n",
    "        loss_epoch_mean = np.array(loss_epoch).mean()\n",
    "        history[repeat_idx,epoch_idx,0] = loss_epoch_mean\n",
    "        \n",
    "        # test\n",
    "        model.eval()\n",
    "        \n",
    "        correct = 0\n",
    "        for batch in dataloader_conv:\n",
    "            y_hat = model(batch[0]).argmax(dim=1)\n",
    "            correct += int((y_hat == batch[1]).sum())\n",
    "        accuracy_train = correct / len(dataloader_conv.dataset)\n",
    "        history[repeat_idx,epoch_idx,1] = accuracy_train\n",
    "        \n",
    "        print(\"{:3} {:.4f} {:.4f}\".format(epoch_idx, loss_epoch_mean, accuracy_train))\n",
    "    print(\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ec1c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
